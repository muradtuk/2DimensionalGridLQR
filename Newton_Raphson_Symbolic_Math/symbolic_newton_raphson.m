function [xopt,num_iter,status_code,status_message] = symbolic_newton_raphson(X, Xinit, grad, hess)
% Newton Raphson Optimization (Minimization) using symbolic math
%
% The Newton-Raphson optimization method attempts to minimizes a target function
% by zeroing its gradient. This method is highly efficient, especially
% for convex or semi-convex functions, but requires explicit expressions
% of the gradient vector and Hessian matrix. Direct calculation of these
% derivatives may be tidious in many cases.
% This function simplifies the Newton Raphson algorithm by calculating
% these derivatives automatically using symbolic math.
% To use the function, all one has to do is to create a symbolic function.
% The software will compute the derivatives automatically, and execute
% the Newton Raphson algorithm to find a minimum point.
% For further information please refer to the example
% in 'Newton_Raphson_Symbolic_Math_Example'.
%
% Inputs:
% X - symbolic vector of variables
% Xinit - an initial solution (often selected arbitrarily)
% grad - symbolic gradient vector. Automatically generated by the function 'symbolic_gradient_hessian'
% hess - symbolic Hessian matrix. Automatically generated by the function 'symbolic_gradient_hessian'
%
% Outputs:
% xopt - The minimum point produced by the algorithm
% num_iter - number of iterations done until convergence
% status_code - Reports the results (success / failure)
% status_message - Reports the results (success / failure)
%
% Written by Dr. Yoash Levron, Technion, Israel, 2015

% Parameters (may be tuned by the user for i    ncreased accuracy / faster run time)
epsilon = 1e-3;   % stopping criterion (tolerable error in gradient norm)
max_iterations = 100;  % maximum number of iterations

% Initialize
N = size(grad,1);
xval = Xinit;   err = inf;  step = NaN(N,1);
count_iterations = 0;
gradval = double(subs(grad,X,xval)); % compute gradient
hessval = double(subs(hess,X,xval)); % compute hessian

% Newton Raphson Iterations
while ((err>epsilon) & (count_iterations<=max_iterations))
    rnk = rank(hessval);  % rank of Hessian matrix
    if (rnk == N)
        step = hessval\gradval; % step in solution X
    else % if Hessian is not full rank
        hess_p = hessval(:,1:rnk); % take first columns
        step(1:rnk) = hess_p\gradval;
        step((rnk+1):N) = zeros(N-rnk,1);
    end
    ind = find(isnan(step) | isinf(step));
    if ~isempty(ind)
        break;
    end
    xval = xval - step; % new value of X
    gradval = double(subs(grad,X,xval));  % compute gradient
    hessval= double(subs(hess,X,xval)); % compute hessian
    err = ((gradval')*gradval)^0.5; % compute norm of gradient
    count_iterations = count_iterations+1;
end
xopt = xval;
num_iter = count_iterations;

% Report results
if (err<=epsilon)
    status_message = 'success - algorithm converged (possibly to a local minimum)';
    status_code = 0;
elseif (count_iterations > max_iterations)
    status_message = 'failure - max. number of iterations exceeded';
    status_code = 1;
else
    fprintf('Error is %f\n', err);
    status_message = 'failure - hessian matrix cannot be inverted';
    status_code = 2;
end

end

